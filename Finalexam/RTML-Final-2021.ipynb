{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Final 2021\n",
    "\n",
    "In this exam, we'll have some practical exercises using RNNs and some short answer questions regarding the Transformer/attention\n",
    "and reinforcement learning.\n",
    "\n",
    "Consider the AGNews text classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label frequencies: {3: 30000, 4: 30000, 2: 30000, 1: 30000}\nA few token frequencies: [('.', 225971), ('the', 205040), (',', 165685), ('to', 119817), ('a', 110942)]\nLabel meanings: 1: World news, 2: Sports news, 3: Business news, 4: Sci/Tech news\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace('\\\\', ' ')\n",
    "    return line\n",
    "\n",
    "labels = {}\n",
    "for (label, line) in train_iter:\n",
    "    if label in labels:\n",
    "        labels[label] += 1\n",
    "    else:\n",
    "        labels[label] = 1\n",
    "    counter.update(tokenizer(clean(line)))\n",
    "\n",
    "vocab = Vocab(counter, min_freq=1)\n",
    "\n",
    "print('Label frequencies:', labels)\n",
    "print('A few token frequencies:', vocab.freqs.most_common(5))\n",
    "print('Label meanings: 1: World news, 2: Sports news, 3: Business news, 4: Sci/Tech news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can get a sequence of tokens for a sentence with the cleaner, tokenizer, and vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[4248, 4, 116, 3, 244, 46857, 4, 23, 62, 7, 3, 812, 2009, 7, 989]"
      ]
     },
     "metadata": {},
     "execution_count": 133
    }
   ],
   "source": [
    "[vocab[token] for token in tokenizer(clean('Bangkok, or The Big Mango, is one of the great cities of Asia'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make pipelines for processing a news story and a label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(clean(x))]\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to create dataloaders for the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        length_list.append(processed_text.shape[0])\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, padding_value=0)\n",
    "    length_list = torch.tensor(length_list, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device), length_list.to(device)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "train_dataset = list(train_iter)\n",
    "test_iter = AG_NEWS(split='test')\n",
    "test_dataset = list(test_iter)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to get a batch from one of these dataloaders. The first entry is a 1D tensor of labels for the batch\n",
    "(8 values between 0 and 3), then a 2D tensor representing the stories with dimension T x B (number of tokens x batch size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, (tensor([3, 2, 0, 2, 1, 2, 1, 0], device='cuda:0'), tensor([[ 7830,   342,   119,   369,  1447,   342, 16255, 28640],\n        [ 4049, 54733,   949,   170,   130,   219,    90,  1682],\n        [12159,  1112,    37,  2424, 13187,   386,  2590,    48],\n        [  929,     7,   475,     5,     4,  4233,    14,   119],\n        [  884,  1550,   121,  2539,   267, 20173,   263, 15086],\n        [   14,   256,     2,    39,  1406,  1367,   203,     8],\n        [  406,     4, 12760,   309,    14,   122,    15, 49579],\n        [   51,   592,  1041,  5134,  2202, 15144,    16,    14],\n        [   15,  2778,     7,    19,    15,   206,     3,    32],\n        [  406,  8681,  2533,   309,    54, 11582,  1739,    15],\n        [   51,  6621,    12,    13,    49, 11101,    40,    32],\n        [   16,    66,  1158,    10,     7,    39,    83,    16],\n        [ 1239,     2,  1595,  4977,     3,    78,  3873,     3],\n        [ 2939,    14,  1134,     2,   132,    88,     8,  4599],\n        [47525, 38238,     4,   121,  2884,     2,    48,    23],\n        [  172,     2,   684,   139,     9,     4,   585,  9511],\n        [    5,   374,  1947,  4514,  1477,   169,     8,     4],\n        [48535,   491,    14,   375,  2850,   282,     3,   280],\n        [    6,     4,   366,    12,   111,  1778,   185, 64971],\n        [  406,   563,   329,   369,    91,  1014,   176,    23],\n        [   17,     4,    15,   165,    80,    27,   332, 10844],\n        [   10,   208,   366,     5,     2,    57,  1019,    11],\n        [ 8130,    15,   329,  8789,   565,    68,   133,     3],\n        [    2,    11,    16,    65,  1447,    79,   252, 12398],\n        [    0,    56,   845,     4,   130,  2323,  1225,     9],\n        [    0,   678,    14,   721, 39634,   578,   446,   365],\n        [    0,     7,    32,     6,    19,     3,     2, 77982],\n        [    0,    31,    15,     2, 14277,   108,    18, 23655],\n        [    0,   271,    16, 12853,  5595,   247,    94,  1040],\n        [    0,  7176,   119,     4,    21,    13,   647,   622],\n        [    0,     4,   731,   369, 20867,    10,  1380,   510],\n        [    0,   136,  2533,    13,  8952,   122,     4,    43],\n        [    0,     3,    56,    10,   709,    19,    20,  2263],\n        [    0,  2726,  1041,   170,    11,   418,   579,     3],\n        [    0,     7,   398,   325,    58,  4854,   611, 67253],\n        [    0,    22,   475,     4,     8,   102,  2230,     2],\n        [    0,   170,   121,   637,     3,  1367,     5,     0],\n        [    0,   325,     2,   309, 20711,    66,  1263,     0],\n        [    0,     9,   109,    11,  2043,     2,   278,     0],\n        [    0,    27,   139,    59,   945,     0,  4051,     0],\n        [    0,    26,    37,     7,     2,     0,     8,     0],\n        [    0,    36,     5,  5177,     0,     0,     6,     0],\n        [    0,     3,   277,     0,     0,     0,  3413,     0],\n        [    0,  3294,  1158,     0,     0,     0,   235,     0],\n        [    0,     0,  1595,     0,     0,     0,    25,     0],\n        [    0,     0, 13635,     0,     0,     0,     3,     0],\n        [    0,     0,     9,     0,     0,     0,  3941,     0],\n        [    0,     0,   158,     0,     0,     0,   229,     0],\n        [    0,     0,   684,     0,     0,     0,  1738,     0],\n        [    0,     0,  1703,     0,     0,     0,  3823,     0],\n        [    0,     0,  1947,     0,     0,     0,    21,     0],\n        [    0,     0,     2,     0,     0,     0, 22801,     0],\n        [    0,     0,     0,     0,     0,     0,   960,     0],\n        [    0,     0,     0,     0,     0,     0,     2,     0]],\n       device='cuda:0'), tensor([24, 44, 52, 42, 41, 39, 54, 36], device='cuda:0')))\n"
     ]
    }
   ],
   "source": [
    "batch = next(enumerate(train_dataloader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1, 10 points\n",
    "\n",
    "The vocabulary currently is too large for a simple one-hot embedding. Let's reduce the vocabulary size\n",
    "so that we can use one-hot. First, add a step that removes tokens from a list of \"stop words\" to the `text_pipeline` function.\n",
    "You probably want to remove punctuation ('.', ',', '-', etc.) and articles (\"a\", \"the\").\n",
    "\n",
    "Once you've removed stop words, modify the vocabulary to include only the most frequent 1000 tokens (including 0 for an unknown/infrequent word).\n",
    "\n",
    "Write your revised code in the cell below and output the 999 top words with their frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place code for Question 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('.', 225971),\n",
       " ('the', 205040),\n",
       " (',', 165685),\n",
       " ('to', 119817),\n",
       " ('a', 110942),\n",
       " ('of', 98353),\n",
       " ('in', 95930),\n",
       " ('and', 69326),\n",
       " ('s', 61915),\n",
       " ('on', 57279),\n",
       " ('for', 50417),\n",
       " ('#39', 44316),\n",
       " ('(', 41106),\n",
       " (')', 40787),\n",
       " ('-', 39212),\n",
       " (\"'\", 32235),\n",
       " ('that', 28167),\n",
       " ('with', 26801),\n",
       " ('as', 25324),\n",
       " ('at', 24999),\n",
       " ('its', 22115),\n",
       " ('is', 22083),\n",
       " ('new', 21297),\n",
       " ('by', 20858),\n",
       " ('it', 20476),\n",
       " ('said', 20265),\n",
       " ('reuters', 19321),\n",
       " ('has', 19023),\n",
       " ('from', 17807),\n",
       " ('an', 16988),\n",
       " ('ap', 16152),\n",
       " ('his', 14941),\n",
       " ('will', 14615),\n",
       " ('after', 14496),\n",
       " ('was', 13730),\n",
       " ('us', 12660),\n",
       " ('be', 11777),\n",
       " ('over', 11219),\n",
       " ('have', 11200),\n",
       " ('their', 10529),\n",
       " ('&lt', 10208),\n",
       " ('are', 9791),\n",
       " ('up', 9739),\n",
       " ('quot', 9593),\n",
       " ('but', 9150),\n",
       " ('more', 9123),\n",
       " ('first', 9087),\n",
       " ('two', 8974),\n",
       " ('he', 8920),\n",
       " ('world', 8522),\n",
       " ('u', 8357),\n",
       " ('this', 8246),\n",
       " ('--', 7969),\n",
       " ('company', 7637),\n",
       " ('monday', 7614),\n",
       " ('wednesday', 7530),\n",
       " ('tuesday', 7454),\n",
       " ('thursday', 7343),\n",
       " ('oil', 7262),\n",
       " ('out', 7207),\n",
       " ('one', 7107),\n",
       " ('not', 6998),\n",
       " ('against', 6899),\n",
       " ('friday', 6869),\n",
       " ('inc', 6853),\n",
       " ('into', 6675),\n",
       " ('they', 6450),\n",
       " ('about', 6412),\n",
       " ('last', 6340),\n",
       " ('iraq', 6306),\n",
       " ('than', 6266),\n",
       " ('year', 6257),\n",
       " ('york', 6169),\n",
       " ('yesterday', 6099),\n",
       " ('who', 6080),\n",
       " ('president', 5912),\n",
       " ('microsoft', 5872),\n",
       " ('were', 5811),\n",
       " ('no', 5797),\n",
       " ('?', 5699),\n",
       " ('million', 5544),\n",
       " ('been', 5533),\n",
       " ('t', 5423),\n",
       " ('says', 5344),\n",
       " ('had', 5279),\n",
       " ('week', 5275),\n",
       " ('corp', 5169),\n",
       " ('united', 5117),\n",
       " ('game', 5088),\n",
       " ('when', 5018),\n",
       " ('sunday', 4924),\n",
       " ('prices', 4906),\n",
       " ('could', 4852),\n",
       " ('three', 4828),\n",
       " ('would', 4788),\n",
       " ('government', 4722),\n",
       " ('years', 4702),\n",
       " ('group', 4701),\n",
       " ('security', 4700),\n",
       " ('today', 4699)]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "vocab.freqs.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace('\\\\', ' ')\n",
    "    line = line.replace('.', ' ')\n",
    "    line = line.replace('the', ' ')\n",
    "    line = line.replace(',', ' ')\n",
    "    line = line.replace('/', ' ')\n",
    "    line = line.replace('=', ' ')\n",
    "    # line = line.replace('a', ' ') # replace these will break a word\n",
    "    # line = line.replace('s', ' ')\n",
    "    line = line.replace('#39', ' ')\n",
    "    line = line.replace(')', ' ')\n",
    "    line = line.replace('(', ' ')\n",
    "    line = line.replace('-', ' ')\n",
    "    line = line.replace(\"'\", ' ')\n",
    "    # line = line.replace('an', ' ')\n",
    "    line = line.replace('--', ' ')\n",
    "    return line\n",
    "\n",
    "def get_freq(batch):\n",
    "    for _, _line in batch:\n",
    "        counter.update(tokenizer(clean(_line)))\n",
    "    vocab = Vocab(counter, min_freq=1)\n",
    "    return vocab.freqs.most_common(999)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    text_pipeline = lambda x: [vocab[token] for token in tokenizer(clean(x))]\n",
    "    label_pipeline = lambda x: int(x) - 1\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        length_list.append(processed_text.shape[0])\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, padding_value=0)\n",
    "    length_list = torch.tensor(length_list, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device), length_list.to(device)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "train_dataset = list(train_iter)\n",
    "train_freq = get_freq(train_dataset)\n",
    "list_t = []\n",
    "for i in range(len(train_freq)):\n",
    "    list_t.append(train_freq[i][0])\n",
    "test_iter = AG_NEWS(split='test')\n",
    "test_dataset = list(test_iter)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([67, 8])\n"
     ]
    }
   ],
   "source": [
    "batch = next(enumerate(train_dataloader))\n",
    "print(batch[1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('to', 120681),\n",
       " ('a', 112141),\n",
       " ('of', 98655),\n",
       " ('in', 96429),\n",
       " ('and', 69678),\n",
       " ('s', 62049),\n",
       " ('on', 57694),\n",
       " ('for', 50676),\n",
       " ('that', 28169),\n",
       " ('with', 26813),\n",
       " ('the', 26472),\n",
       " ('as', 25375),\n",
       " ('at', 25070),\n",
       " ('its', 22123),\n",
       " ('is', 22095),\n",
       " ('new', 21427),\n",
       " ('by', 20952),\n",
       " ('it', 20537),\n",
       " ('said', 20267),\n",
       " ('reuters', 19332),\n",
       " ('has', 19025),\n",
       " ('from', 17829),\n",
       " ('an', 17047),\n",
       " ('ap', 16253),\n",
       " ('his', 14942),\n",
       " ('will', 14617),\n",
       " ('after', 14554),\n",
       " ('was', 13731),\n",
       " ('us', 13232),\n",
       " ('be', 11853),\n",
       " ('over', 11356),\n",
       " ('have', 11213),\n",
       " ('up', 10740),\n",
       " ('ir', 10352),\n",
       " ('r', 10265),\n",
       " ('two', 10226),\n",
       " ('&lt', 10209),\n",
       " ('first', 9804),\n",
       " ('are', 9792),\n",
       " ('year', 9772),\n",
       " ('quot', 9596),\n",
       " ('but', 9184),\n",
       " ('more', 9149),\n",
       " ('he', 8942),\n",
       " ('world', 8628),\n",
       " ('u', 8436),\n",
       " ('this', 8251),\n",
       " ('one', 8109),\n",
       " ('company', 7657),\n",
       " ('monday', 7616),\n",
       " ('oil', 7564),\n",
       " ('out', 7556),\n",
       " ('wednesday', 7531),\n",
       " ('tuesday', 7455),\n",
       " ('thursday', 7345),\n",
       " ('not', 7061),\n",
       " ('1', 7008),\n",
       " ('against', 6901),\n",
       " ('friday', 6870),\n",
       " ('inc', 6853),\n",
       " ('than', 6733),\n",
       " ('into', 6677),\n",
       " ('2', 6608),\n",
       " ('last', 6548),\n",
       " ('about', 6428),\n",
       " ('iraq', 6335),\n",
       " ('york', 6268),\n",
       " ('yesterday', 6099),\n",
       " ('who', 6086),\n",
       " ('three', 6035),\n",
       " ('president', 5994),\n",
       " ('b&gt', 5967),\n",
       " ('no', 5951),\n",
       " ('microsoft', 5936),\n",
       " ('were', 5811),\n",
       " ('y', 5789),\n",
       " ('game', 5774),\n",
       " ('million', 5760),\n",
       " ('?', 5699),\n",
       " ('week', 5653),\n",
       " ('t', 5620),\n",
       " ('o', 5554),\n",
       " ('been', 5534),\n",
       " ('time', 5498),\n",
       " ('says', 5345),\n",
       " ('had', 5279),\n",
       " ('corp', 5170),\n",
       " ('united', 5129),\n",
       " ('when', 5026),\n",
       " ('stocks', 4952),\n",
       " ('sunday', 4930),\n",
       " ('prices', 4907),\n",
       " ('government', 4862),\n",
       " ('could', 4854),\n",
       " ('com', 4844),\n",
       " ('would', 4821),\n",
       " ('security', 4790),\n",
       " ('years', 4719),\n",
       " ('group', 4711),\n",
       " ('today', 4708),\n",
       " ('people', 4676),\n",
       " ('off', 4662),\n",
       " ('which', 4606),\n",
       " ('may', 4586),\n",
       " ('second', 4546),\n",
       " ('afp', 4529),\n",
       " ('percent', 4514),\n",
       " ('back', 4492),\n",
       " ('software', 4480),\n",
       " ('all', 4455),\n",
       " ('next', 4392),\n",
       " ('3', 4376),\n",
       " ('season', 4345),\n",
       " ('team', 4305),\n",
       " ('day', 4272),\n",
       " ('win', 4267),\n",
       " ('third', 4245),\n",
       " ('n', 4238),\n",
       " ('internet', 4085),\n",
       " ('saturday', 4044),\n",
       " ('or', 4015),\n",
       " ('night', 4014),\n",
       " ('quarter', 3997),\n",
       " ('china', 3968),\n",
       " ('high', 3962),\n",
       " ('top', 3893),\n",
       " ('state', 3876),\n",
       " ('6', 3847),\n",
       " ('market', 3798),\n",
       " ('can', 3795),\n",
       " ('deal', 3793),\n",
       " ('some', 3764),\n",
       " ('sales', 3741),\n",
       " ('open', 3734),\n",
       " ('four', 3715),\n",
       " ('minister', 3714),\n",
       " ('bush', 3678),\n",
       " ('4', 3604),\n",
       " ('billion', 3598),\n",
       " ('record', 3592),\n",
       " ('down', 3572),\n",
       " ('business', 3562),\n",
       " ('end', 3542),\n",
       " ('2004', 3539),\n",
       " ('news', 3461),\n",
       " ('announced', 3445),\n",
       " ('international', 3426),\n",
       " ('washington', 3389),\n",
       " ('most', 3385),\n",
       " ('former', 3381),\n",
       " ('m', 3353),\n",
       " ('killed', 3353),\n",
       " ('10', 3337),\n",
       " ('profit', 3256),\n",
       " ('5', 3255),\n",
       " ('report', 3242),\n",
       " ('victory', 3225),\n",
       " ('officials', 3223),\n",
       " ('city', 3199),\n",
       " ('court', 3198),\n",
       " ('service', 3161),\n",
       " ('000', 3142),\n",
       " ('home', 3141),\n",
       " ('plans', 3134),\n",
       " ('month', 3117),\n",
       " ('set', 3087),\n",
       " ('if', 3077),\n",
       " ('states', 3052),\n",
       " ('you', 3030),\n",
       " ('european', 3025),\n",
       " ('before', 3008),\n",
       " ('chief', 2998),\n",
       " ('american', 2985),\n",
       " ('technology', 2957),\n",
       " ('lead', 2933),\n",
       " ('7', 2915),\n",
       " ('search', 2898),\n",
       " ('computer', 2896),\n",
       " ('red', 2889),\n",
       " ('league', 2889),\n",
       " ('talks', 2877),\n",
       " ('cup', 2849),\n",
       " ('0', 2840),\n",
       " ('just', 2838),\n",
       " ('space', 2818),\n",
       " ('online', 2810),\n",
       " ('re', 2809),\n",
       " ('five', 2808),\n",
       " ('country', 2777),\n",
       " ('what', 2745),\n",
       " ('now', 2731),\n",
       " ('national', 2729),\n",
       " ('co', 2722),\n",
       " ('british', 2722),\n",
       " ('expected', 2713),\n",
       " ('largest', 2709),\n",
       " ('reported', 2706),\n",
       " ('take', 2698),\n",
       " ('shares', 2671),\n",
       " ('india', 2670),\n",
       " ('target', 2658),\n",
       " ('bank', 2639),\n",
       " ('japan', 2631),\n",
       " ('won', 2627),\n",
       " ('federal', 2624),\n",
       " ('prime', 2612),\n",
       " ('google', 2611),\n",
       " ('major', 2602),\n",
       " ('network', 2598),\n",
       " ('final', 2593),\n",
       " ('ibm', 2577),\n",
       " ('police', 2570),\n",
       " ('under', 2566),\n",
       " ('least', 2566),\n",
       " ('make', 2562),\n",
       " ('iraqi', 2545),\n",
       " ('election', 2539),\n",
       " ('web', 2533),\n",
       " ('hit', 2530),\n",
       " ('ano', 2524),\n",
       " ('research', 2523),\n",
       " ('south', 2522),\n",
       " ('music', 2511),\n",
       " ('made', 2503),\n",
       " ('according', 2489),\n",
       " ('maker', 2479),\n",
       " ('long', 2477),\n",
       " ('bid', 2457),\n",
       " ('leader', 2456),\n",
       " ('companies', 2451),\n",
       " ('help', 2436),\n",
       " ('get', 2435),\n",
       " ('big', 2429),\n",
       " ('mobile', 2423),\n",
       " ('while', 2415),\n",
       " ('games', 2406),\n",
       " ('during', 2403),\n",
       " ('san', 2393),\n",
       " ('london', 2384),\n",
       " ('rn', 2383),\n",
       " ('series', 2377),\n",
       " ('coach', 2368),\n",
       " ('between', 2362),\n",
       " ('still', 2357),\n",
       " ('plan', 2352),\n",
       " ('ticker', 2343),\n",
       " ('industry', 2342),\n",
       " ('say', 2342),\n",
       " ('way', 2305),\n",
       " ('baghdad', 2289),\n",
       " ('war', 2271),\n",
       " ('http', 2269),\n",
       " ('dollar', 2264),\n",
       " ('old', 2254),\n",
       " ('run', 2240),\n",
       " ('giant', 2237),\n",
       " ('based', 2215),\n",
       " ('growth', 2199),\n",
       " ('services', 2188),\n",
       " ('al', 2188),\n",
       " ('since', 2186),\n",
       " ('www', 2184),\n",
       " ('i', 2181),\n",
       " ('through', 2179),\n",
       " ('early', 2169),\n",
       " ('data', 2148),\n",
       " ('north', 2145),\n",
       " ('investor', 2130),\n",
       " ('her', 2126),\n",
       " ('nuclear', 2125),\n",
       " ('wireless', 2124),\n",
       " ('sports', 2119),\n",
       " ('href', 2119),\n",
       " ('a&gt', 2117),\n",
       " ('start', 2116),\n",
       " ('system', 2115),\n",
       " ('trade', 2112),\n",
       " ('higher', 2111),\n",
       " ('cut', 2107),\n",
       " ('phone', 2099),\n",
       " ('military', 2076),\n",
       " ('left', 2076),\n",
       " ('six', 2074),\n",
       " ('australia', 2073),\n",
       " ('gold', 2065),\n",
       " ('earnings', 2057),\n",
       " ('test', 2053),\n",
       " ('union', 2049),\n",
       " ('buy', 2048),\n",
       " ('so', 2039),\n",
       " ('being', 2032),\n",
       " ('john', 2026),\n",
       " ('only', 2020),\n",
       " ('un', 2007),\n",
       " ('move', 2000),\n",
       " ('stock', 1992),\n",
       " ('him', 1992),\n",
       " ('palestinian', 1978),\n",
       " ('general', 1974),\n",
       " ('loss', 1969),\n",
       " ('like', 1966),\n",
       " ('players', 1961),\n",
       " ('official', 1957),\n",
       " ('apple', 1925),\n",
       " ('amp', 1922),\n",
       " ('months', 1920),\n",
       " ('agreed', 1911),\n",
       " ('sox', 1907),\n",
       " ('go', 1902),\n",
       " ('days', 1899),\n",
       " ('half', 1896),\n",
       " ('man', 1883),\n",
       " ('oracle', 1881),\n",
       " ('because', 1880),\n",
       " ('air', 1879),\n",
       " ('8', 1872),\n",
       " ('attack', 1862),\n",
       " ('israeli', 1858),\n",
       " ('face', 1855),\n",
       " ('windows', 1849),\n",
       " ('many', 1846),\n",
       " ('ahead', 1841),\n",
       " ('latest', 1835),\n",
       " ('global', 1824),\n",
       " ('olympic', 1822),\n",
       " ('uk', 1818),\n",
       " ('troops', 1815),\n",
       " ('fullquote', 1813),\n",
       " ('aspx', 1813),\n",
       " ('quickinfo', 1813),\n",
       " ('fullquote&gt', 1813),\n",
       " ('price', 1809),\n",
       " ('england', 1807),\n",
       " ('intel', 1804),\n",
       " ('play', 1802),\n",
       " ('gaza', 1802),\n",
       " ('russia', 1801),\n",
       " ('again', 1799),\n",
       " ('biggest', 1794),\n",
       " ('free', 1791),\n",
       " ('firm', 1785),\n",
       " ('executive', 1785),\n",
       " ('west', 1782),\n",
       " ('rose', 1781),\n",
       " ('round', 1781),\n",
       " ('found', 1774),\n",
       " ('held', 1769),\n",
       " ('even', 1762),\n",
       " ('press', 1761),\n",
       " ('near', 1756),\n",
       " ('head', 1748),\n",
       " ('jobs', 1746),\n",
       " ('users', 1746),\n",
       " ('drug', 1745),\n",
       " ('11', 1739),\n",
       " ('including', 1738),\n",
       " ('pay', 1737),\n",
       " ('boston', 1736),\n",
       " ('russian', 1735),\n",
       " ('iran', 1731),\n",
       " ('rise', 1731),\n",
       " ('ago', 1723),\n",
       " ('reports', 1718),\n",
       " ('update', 1716),\n",
       " ('football', 1709),\n",
       " ('released', 1701),\n",
       " ('despite', 1699),\n",
       " ('economy', 1690),\n",
       " ('points', 1683),\n",
       " ('part', 1680),\n",
       " ('europe', 1679),\n",
       " ('20', 1675),\n",
       " ('peoplesoft', 1672),\n",
       " ('car', 1670),\n",
       " ('much', 1670),\n",
       " ('forces', 1662),\n",
       " ('how', 1660),\n",
       " ('e', 1659),\n",
       " ('investors', 1656),\n",
       " ('past', 1655),\n",
       " ('economic', 1645),\n",
       " ('release', 1643),\n",
       " ('peace', 1643),\n",
       " ('canadian', 1640),\n",
       " ('nor', 1634),\n",
       " ('power', 1631),\n",
       " ('street', 1619),\n",
       " ('key', 1618),\n",
       " ('9', 1609),\n",
       " ('pakistan', 1608),\n",
       " ('eu', 1601),\n",
       " ('offer', 1599),\n",
       " ('work', 1595),\n",
       " ('your', 1589),\n",
       " ('2005', 1587),\n",
       " ('video', 1587),\n",
       " ('beat', 1584),\n",
       " ('use', 1580),\n",
       " ('strong', 1580),\n",
       " ('public', 1569),\n",
       " ('seven', 1565),\n",
       " ('case', 1564),\n",
       " ('fourth', 1558),\n",
       " ('nation', 1557),\n",
       " ('source', 1549),\n",
       " ('presidential', 1540),\n",
       " ('bomb', 1538),\n",
       " ('share', 1535),\n",
       " ('foreign', 1527),\n",
       " ('where', 1527),\n",
       " ('right', 1520),\n",
       " ('title', 1519),\n",
       " ('sou', 1517),\n",
       " ('should', 1517),\n",
       " ('nations', 1515),\n",
       " ('weeks', 1515),\n",
       " ('12', 1514),\n",
       " ('nearly', 1511),\n",
       " ('pc', 1503),\n",
       " ('sun', 1503),\n",
       " ('close', 1502),\n",
       " ('do', 1496),\n",
       " ('around', 1494),\n",
       " ('tokyo', 1494),\n",
       " ('called', 1492),\n",
       " ('america', 1488),\n",
       " ('workers', 1488),\n",
       " ('israel', 1484),\n",
       " ('linux', 1483),\n",
       " ('lower', 1480),\n",
       " ('attacks', 1477),\n",
       " ('net', 1475),\n",
       " ('life', 1475),\n",
       " ('financial', 1473),\n",
       " ('systems', 1473),\n",
       " ('wins', 1471),\n",
       " ('media', 1465),\n",
       " ('support', 1464),\n",
       " ('french', 1463),\n",
       " ('nasa', 1463),\n",
       " ('australian', 1454),\n",
       " ('kerry', 1450),\n",
       " ('house', 1447),\n",
       " ('championship', 1445),\n",
       " ('korea', 1441),\n",
       " ('agency', 1435),\n",
       " ('launch', 1434),\n",
       " ('contract', 1432),\n",
       " ('st', 1426),\n",
       " ('best', 1426),\n",
       " ('wall', 1424),\n",
       " ('fall', 1421),\n",
       " ('francisco', 1421),\n",
       " ('low', 1418),\n",
       " ('leaders', 1415),\n",
       " ('digital', 1412),\n",
       " ('put', 1407),\n",
       " ('men', 1402),\n",
       " ('late', 1401),\n",
       " ('leading', 1400),\n",
       " ('30', 1398),\n",
       " ('player', 1398),\n",
       " ('number', 1394),\n",
       " ('fell', 1387),\n",
       " ('september', 1382),\n",
       " ('october', 1380),\n",
       " ('line', 1379),\n",
       " ('party', 1378),\n",
       " ('demand', 1378),\n",
       " ('chicago', 1376),\n",
       " ('crude', 1371),\n",
       " ('used', 1371),\n",
       " ('also', 1370),\n",
       " ('led', 1369),\n",
       " ('death', 1369),\n",
       " ('took', 1366),\n",
       " ('scientists', 1362),\n",
       " ('florida', 1362),\n",
       " ('hurricane', 1351),\n",
       " ('any', 1347),\n",
       " ('good', 1346),\n",
       " ('rival', 1345),\n",
       " ('following', 1345),\n",
       " ('job', 1343),\n",
       " ('race', 1342),\n",
       " ('consumer', 1340),\n",
       " ('killing', 1340),\n",
       " ('return', 1334),\n",
       " ('15', 1333),\n",
       " ('arafat', 1333),\n",
       " ('darfur', 1330),\n",
       " ('chip', 1329),\n",
       " ('per', 1320),\n",
       " ('capital', 1320),\n",
       " ('army', 1318),\n",
       " ('program', 1317),\n",
       " ('vote', 1314),\n",
       " ('center', 1313),\n",
       " ('here', 1309),\n",
       " ('version', 1309),\n",
       " ('#36', 1307),\n",
       " ('charges', 1307),\n",
       " ('france', 1305),\n",
       " ('keep', 1301),\n",
       " ('japanese', 1297),\n",
       " ('rs', 1296),\n",
       " ('commission', 1294),\n",
       " ('yankees', 1291),\n",
       " ('agreement', 1289),\n",
       " ('campaign', 1289),\n",
       " ('school', 1288),\n",
       " ('future', 1287),\n",
       " ('trial', 1287),\n",
       " ('bill', 1286),\n",
       " ('both', 1284),\n",
       " ('quote', 1278),\n",
       " ('site', 1267),\n",
       " ('well', 1263),\n",
       " ('making', 1262),\n",
       " ('star', 1261),\n",
       " ('we', 1257),\n",
       " ('products', 1256),\n",
       " ('strike', 1255),\n",
       " ('profile', 1252),\n",
       " ('dead', 1250),\n",
       " ('region', 1248),\n",
       " ('give', 1247),\n",
       " ('anti', 1244),\n",
       " ('study', 1242),\n",
       " ('n&lt', 1241),\n",
       " ('17', 1240),\n",
       " ('show', 1239),\n",
       " ('britain', 1238),\n",
       " ('board', 1237),\n",
       " ('women', 1234),\n",
       " ('management', 1228),\n",
       " ('tech', 1226),\n",
       " ('customers', 1223),\n",
       " ('might', 1222),\n",
       " ('defense', 1220),\n",
       " ('results', 1220),\n",
       " ('battle', 1219),\n",
       " ('away', 1217),\n",
       " ('conference', 1217),\n",
       " ('decision', 1215),\n",
       " ('several', 1214),\n",
       " ('energy', 1214),\n",
       " ('office', 1211),\n",
       " ('november', 1210),\n",
       " ('little', 1207),\n",
       " ('14', 1205),\n",
       " ('$1', 1205),\n",
       " ('eight', 1199),\n",
       " ('sony', 1196),\n",
       " ('manager', 1195),\n",
       " ('costs', 1191),\n",
       " ('place', 1191),\n",
       " ('meeting', 1191),\n",
       " ('without', 1189),\n",
       " ('political', 1187),\n",
       " ('militants', 1185),\n",
       " ('sudan', 1183),\n",
       " ('shot', 1183),\n",
       " ('hostage', 1183),\n",
       " ('running', 1181),\n",
       " ('18', 1181),\n",
       " ('sell', 1177),\n",
       " ('elections', 1173),\n",
       " ('gets', 1172),\n",
       " ('takes', 1171),\n",
       " ('california', 1165),\n",
       " ('baseball', 1165),\n",
       " ('champions', 1164),\n",
       " ('health', 1162),\n",
       " ('field', 1161),\n",
       " ('recent', 1160),\n",
       " ('judge', 1158),\n",
       " ('taking', 1157),\n",
       " ('cost', 1154),\n",
       " ('champion', 1153),\n",
       " ('cuts', 1152),\n",
       " ('canada', 1152),\n",
       " ('due', 1151),\n",
       " ('real', 1150),\n",
       " ('fight', 1149),\n",
       " ('tax', 1148),\n",
       " ('13', 1146),\n",
       " ('interest', 1145),\n",
       " ('winning', 1145),\n",
       " ('central', 1138),\n",
       " ('such', 1137),\n",
       " ('straight', 1137),\n",
       " ('match', 1136),\n",
       " ('quarterly', 1133),\n",
       " ('force', 1131),\n",
       " ('16', 1129),\n",
       " ('mail', 1127),\n",
       " ('fans', 1126),\n",
       " ('hopes', 1125),\n",
       " ('department', 1119),\n",
       " ('told', 1119),\n",
       " ('tv', 1117),\n",
       " ('ceo', 1116),\n",
       " ('claims', 1112),\n",
       " ('she', 1111),\n",
       " ('los', 1110),\n",
       " ('better', 1104),\n",
       " ('whe', 1104),\n",
       " ('michael', 1103),\n",
       " ('rate', 1093),\n",
       " ('using', 1091),\n",
       " ('times', 1090),\n",
       " ('giants', 1087),\n",
       " ('likely', 1083),\n",
       " ('scored', 1083),\n",
       " ('east', 1082),\n",
       " ('increase', 1081),\n",
       " ('grand', 1078),\n",
       " ('across', 1074),\n",
       " ('p&gt', 1074),\n",
       " ('own', 1073),\n",
       " ('launched', 1072),\n",
       " ('violence', 1071),\n",
       " ('look', 1066),\n",
       " ('university', 1066),\n",
       " ('amid', 1065),\n",
       " ('small', 1065),\n",
       " ('mark', 1065),\n",
       " ('lost', 1063),\n",
       " ('saying', 1063),\n",
       " ('angeles', 1063),\n",
       " ('become', 1061),\n",
       " ('texas', 1061),\n",
       " ('25', 1060),\n",
       " ('club', 1059),\n",
       " ('money', 1057),\n",
       " ('action', 1057),\n",
       " ('history', 1055),\n",
       " ('rates', 1051),\n",
       " ('chairman', 1051),\n",
       " ('chinese', 1045),\n",
       " ('trading', 1044),\n",
       " ('unit', 1044),\n",
       " ('fire', 1041),\n",
       " ('houston', 1040),\n",
       " ('williams', 1039),\n",
       " ('stores', 1035),\n",
       " ('weekend', 1034),\n",
       " ('server', 1031),\n",
       " ('hits', 1025),\n",
       " ('among', 1021),\n",
       " ('too', 1020),\n",
       " ('control', 1019),\n",
       " ('germany', 1018),\n",
       " ('100', 1013),\n",
       " ('cash', 1006),\n",
       " ('same', 1006),\n",
       " ('morning', 1006),\n",
       " ('call', 1005),\n",
       " ('rebels', 1005),\n",
       " ('secretary', 1005),\n",
       " ('desktop', 1005),\n",
       " ('come', 1003),\n",
       " ('behind', 1003),\n",
       " ('information', 1002),\n",
       " ('cell', 999),\n",
       " ('oct', 999),\n",
       " ('accused', 996),\n",
       " ('change', 993),\n",
       " ('phones', 992),\n",
       " ('less', 990),\n",
       " ('yahoo', 986),\n",
       " ('soldiers', 984),\n",
       " ('seen', 983),\n",
       " ('going', 983),\n",
       " ('possible', 983),\n",
       " ('german', 982),\n",
       " ('drop', 981),\n",
       " ('aid', 981),\n",
       " ('ipod', 981),\n",
       " ('injured', 981),\n",
       " ('profits', 978),\n",
       " ('afghanistan', 978),\n",
       " ('boost', 977),\n",
       " ('white', 976),\n",
       " ('see', 976),\n",
       " ('operating', 975),\n",
       " ('makes', 975),\n",
       " ('hold', 975),\n",
       " ('meet', 974),\n",
       " ('law', 973),\n",
       " ('communications', 967),\n",
       " ('august', 966),\n",
       " ('exchange', 965),\n",
       " ('senior', 964),\n",
       " ('few', 964),\n",
       " ('ever', 964),\n",
       " ('24', 961),\n",
       " ('revenue', 959),\n",
       " ('store', 958),\n",
       " ('almost', 958),\n",
       " ('calls', 957),\n",
       " ('speed', 955),\n",
       " ('warned', 955),\n",
       " ('bankruptcy', 955),\n",
       " ('barrel', 954),\n",
       " ('reach', 954),\n",
       " ('did', 954),\n",
       " ('nine', 954),\n",
       " ('radio', 951),\n",
       " ('point', 946),\n",
       " ('thousands', 946),\n",
       " ('medal', 945),\n",
       " ('paris', 945),\n",
       " ('hard', 943),\n",
       " ('rally', 942),\n",
       " ('indian', 940),\n",
       " ('nov', 939),\n",
       " ('access', 937),\n",
       " ('full', 936),\n",
       " ('gas', 936),\n",
       " ('airlines', 936),\n",
       " ('takeover', 933),\n",
       " ('posted', 930),\n",
       " ('ready', 928),\n",
       " ('food', 928),\n",
       " ('19', 927),\n",
       " ('reserve', 925),\n",
       " ('euro', 924),\n",
       " ('showed', 923),\n",
       " ('file', 922),\n",
       " ('signed', 918),\n",
       " ('human', 917),\n",
       " ('africa', 917),\n",
       " ('engine', 916),\n",
       " ('fund', 915),\n",
       " ('term', 915),\n",
       " ('those', 914),\n",
       " ('production', 913),\n",
       " ('television', 913),\n",
       " ('coast', 912),\n",
       " ('afghan', 912),\n",
       " ('opening', 911),\n",
       " ('forecast', 911),\n",
       " ('sign', 911),\n",
       " ('must', 911),\n",
       " ('stadium', 911),\n",
       " ('toronto', 910),\n",
       " ('yet', 910),\n",
       " ('terror', 909),\n",
       " ('ns', 909),\n",
       " ('21', 907),\n",
       " ('d', 906),\n",
       " ('earlier', 904),\n",
       " ('tour', 902),\n",
       " ('step', 900),\n",
       " ('jones', 900),\n",
       " ('david', 900),\n",
       " ('nfl', 898),\n",
       " ('short', 897),\n",
       " ('fuel', 897),\n",
       " ('markets', 895),\n",
       " ('growing', 895),\n",
       " ('far', 895),\n",
       " ('offering', 892),\n",
       " ('got', 890),\n",
       " ('stop', 890),\n",
       " ('wants', 889),\n",
       " ('spain', 889),\n",
       " ('division', 888),\n",
       " ('soon', 887),\n",
       " ('post', 884),\n",
       " ('opposition', 882),\n",
       " ('outlook', 881),\n",
       " ('find', 879),\n",
       " ('sale', 876),\n",
       " ('helped', 873),\n",
       " ('performance', 872),\n",
       " ('olympics', 872),\n",
       " ('blue', 872),\n",
       " ('begin', 868),\n",
       " ('stake', 865),\n",
       " ('association', 863),\n",
       " ('probe', 862),\n",
       " ('americans', 861),\n",
       " ('rebel', 859),\n",
       " ('corporate', 859),\n",
       " ('began', 858),\n",
       " ('concerns', 856),\n",
       " ('sept', 856),\n",
       " ('ltd', 855),\n",
       " ('blair', 855),\n",
       " ('african', 854),\n",
       " ('local', 852),\n",
       " ('product', 851),\n",
       " ('green', 850),\n",
       " ('came', 849),\n",
       " ('!', 846),\n",
       " ('gains', 846),\n",
       " ('insurance', 845),\n",
       " ('networks', 840),\n",
       " ('threat', 839),\n",
       " ('western', 838),\n",
       " ('flight', 838),\n",
       " ('ended', 837),\n",
       " ('efforts', 837),\n",
       " ('countries', 835),\n",
       " ('legal', 834),\n",
       " ('died', 833),\n",
       " ('administration', 832),\n",
       " ('members', 832),\n",
       " ('career', 832),\n",
       " ('once', 830),\n",
       " ('already', 829),\n",
       " ('station', 829),\n",
       " ('airline', 829),\n",
       " ('investment', 828),\n",
       " ('arrested', 826),\n",
       " ('fighting', 825),\n",
       " ('list', 823),\n",
       " ('analysts', 822),\n",
       " ('looking', 819),\n",
       " ('council', 819),\n",
       " ('miami', 818),\n",
       " ('spending', 817),\n",
       " ('trying', 816),\n",
       " ('hours', 816),\n",
       " ('weapons', 814),\n",
       " ('personal', 813),\n",
       " ('great', 813),\n",
       " ('competition', 812),\n",
       " ('within', 812),\n",
       " ('need', 811),\n",
       " ('teams', 810),\n",
       " ('family', 809),\n",
       " ('arsenal', 809),\n",
       " ('drive', 807),\n",
       " ('holiday', 805),\n",
       " ('visit', 805),\n",
       " ('continue', 804),\n",
       " ('computers', 804),\n",
       " ('madrid', 804),\n",
       " ('supply', 803),\n",
       " ('heart', 801),\n",
       " ('pm', 800),\n",
       " ('rules', 799),\n",
       " ('charged', 799),\n",
       " ('airways', 796),\n",
       " ('spam', 793),\n",
       " ('rights', 793),\n",
       " ('p', 792),\n",
       " ('black', 790),\n",
       " ('don', 788),\n",
       " ('beijing', 788),\n",
       " ('dell', 787),\n",
       " ('outside', 787),\n",
       " ('nba', 787),\n",
       " ('retail', 786),\n",
       " ('george', 784),\n",
       " ('until', 782),\n",
       " ('sharon', 782),\n",
       " ('starting', 779),\n",
       " ('athens', 779),\n",
       " ('pressure', 778),\n",
       " ('crisis', 778),\n",
       " ('they', 777),\n",
       " ('authorities', 774),\n",
       " ('selling', 774),\n",
       " ('enough', 774),\n",
       " ('seattle', 773),\n",
       " ('gave', 770),\n",
       " ('coming', 770),\n",
       " ('50', 770),\n",
       " ('paul', 769),\n",
       " ('offers', 767),\n",
       " ('asia', 766),\n",
       " ('storm', 766),\n",
       " ('consumers', 766),\n",
       " ('development', 765),\n",
       " ('italian', 765),\n",
       " ('finally', 763),\n",
       " ('manchester', 763),\n",
       " ('effort', 761),\n",
       " ('ruling', 761),\n",
       " ('toward', 760),\n",
       " ('bay', 760),\n",
       " ('yards', 759),\n",
       " ('executives', 758),\n",
       " ('reached', 758),\n",
       " ('quarterback', 757),\n",
       " ('injury', 756),\n",
       " ('suicide', 756),\n",
       " ('children', 755),\n",
       " ('fears', 755),\n",
       " ('ban', 753),\n",
       " ('popular', 752),\n",
       " ('each', 752),\n",
       " ('size', 751),\n",
       " ('#151', 750),\n",
       " ('bowl', 750),\n",
       " ('main', 749),\n",
       " ('retailer', 748),\n",
       " ('policy', 747),\n",
       " ('labor', 747),\n",
       " ('sees', 746),\n",
       " ('devices', 746),\n",
       " ('fur', 744),\n",
       " ('c', 744),\n",
       " ('getting', 743),\n",
       " ('side', 743),\n",
       " ('charge', 743),\n",
       " ('survey', 742),\n",
       " ('push', 741),\n",
       " ('taken', 741),\n",
       " ('cisco', 740),\n",
       " ('aimed', 739),\n",
       " ('electronics', 736),\n",
       " ('failed', 736),\n",
       " ('blast', 734),\n",
       " ('name', 733),\n",
       " ('large', 731),\n",
       " ('rivals', 730),\n",
       " ('yukos', 730),\n",
       " ('researchers', 728),\n",
       " ('operations', 728),\n",
       " ('continued', 728),\n",
       " ('shows', 727),\n",
       " ('problems', 726),\n",
       " ('there', 725),\n",
       " ('double', 725),\n",
       " ('private', 724),\n",
       " ('college', 724),\n",
       " ('kill', 723),\n",
       " ('goal', 721),\n",
       " ('known', 720),\n",
       " ('basketball', 720),\n",
       " ('ivan', 720),\n",
       " ('turn', 718),\n",
       " ('powerful', 717),\n",
       " ('rising', 717),\n",
       " ('fraud', 716),\n",
       " ('yasser', 716),\n",
       " ('hope', 713),\n",
       " ('want', 713),\n",
       " ('moscow', 713),\n",
       " ('working', 712),\n",
       " ('fired', 711),\n",
       " ('area', 710),\n",
       " ('soccer', 710),\n",
       " ('missing', 707),\n",
       " ('risk', 706),\n",
       " ('palestinians', 705),\n",
       " ('raised', 705),\n",
       " ('davis', 704),\n",
       " ('named', 704),\n",
       " ('card', 704),\n",
       " ('focus', 703),\n",
       " ('filed', 703),\n",
       " ('calif', 701),\n",
       " ('asian', 699),\n",
       " ('chips', 699),\n",
       " ('atlanta', 699),\n",
       " ('above', 698),\n",
       " ('kills', 698),\n",
       " ('suspected', 697),\n",
       " ('merger', 697),\n",
       " ('level', 696),\n",
       " ('28', 695),\n",
       " ('winter', 695),\n",
       " ('warning', 694),\n",
       " ('leave', 693),\n",
       " ('signs', 693),\n",
       " ('faces', 689),\n",
       " ('firms', 689),\n",
       " ('defeat', 689),\n",
       " ('challenge', 688),\n",
       " ('never', 687),\n",
       " ('terrorism', 687),\n",
       " ('w', 686),\n",
       " ('mission', 684),\n",
       " ('debt', 683),\n",
       " ('27', 683),\n",
       " ('designed', 681),\n",
       " ('muslim', 680),\n",
       " ('23', 679),\n",
       " ('2006', 678),\n",
       " ('annual', 677),\n",
       " ('planned', 677),\n",
       " ('park', 677),\n",
       " ('dvd', 676),\n",
       " ('chance', 675),\n",
       " ('businesses', 674),\n",
       " ('satellite', 673),\n",
       " ('storage', 672),\n",
       " ('town', 671),\n",
       " ('issue', 671),\n",
       " ('investigation', 671),\n",
       " ('try', 670),\n",
       " ('fresh', 669)]"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "train_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2, 30 points\n",
    "\n",
    "Next, let's build a simple RNN for classification of the AGNews dataset. Use a one-hot embedding of the vocabulary\n",
    "entries and the basic RNN from Lab 10. Use the lengths tensor (the third element in the batch returned by the dataloaders)\n",
    "to determine which output to apply the loss to.\n",
    "\n",
    "Place your training code below, and plot the training and test accuracy as a\n",
    "function of epoch. Finally, output a confusion matrix for the test set.\n",
    "\n",
    "*Do not spend a lot of time on the training! A few minutes is enough. The point is to show that the model is\n",
    "learning, not to get the best possible performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place code for Question 2 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class originalRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(originalRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = F.tanh(self.i2h(combined))\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_letters = 8\n",
    "n_hidden = 128\n",
    "n_categories = 4\n",
    "model = originalRNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "def train(category_tensor, line_tensor, model):\n",
    "    hidden = model.initHidden()\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = model(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in model.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-97b20ab092e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 10\n",
    "print_every = 1\n",
    "plot_every = 10\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    for category, line, category_tensor, line_tensor in train_dataloader:\n",
    "        output, loss = train(category_tensor, line_tensor, rnn)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 1000\n",
    "\n",
    "# Just return an output given a line\n",
    "def evaluate(line_tensor, model):\n",
    "    hidden = model.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = model(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output = evaluate(line_tensor, rnn)\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = all_categories.index(category)\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3, 10 points\n",
    "\n",
    "Next, replace the SRNN from Question 2 with a single-layer LSTM. Give the same output (training and testing accuracy as a function of epoch, as well as confusion\n",
    "matrix for the test set). Comment on the differences you observe between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place code for Question 3 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=output_size,)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = F.tanh(self.i2h(combined))\n",
    "        lstm_out, self.hidden = self.lstm(combined.view(len(combined), self.seq_len, -1), self.hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "orirnn = LSTM(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4, 10 points\n",
    "\n",
    "Explain how you could use the Transformer model to perform the same task you explored in Questions 2 and 3.\n",
    "How would attention be useful for this text classification task? Give a precise and detailed answer. Be sure to discuss what\n",
    "parts of the original Transformer you would use and what you would have to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "source": [
    "Fist, transformer see all the line not just word-to-word like RNN and LSTM. If can capture relationship from all word in line which represent as ATTENTION. ATTENTION will tell how much each word important to classify the line. It help classification better since all word has ATTENTION and ATTENTION remove some noise from unnesseary infomation. \n",
    "\n",
    "We can use a TransformerEncoder part to do a text classification. The TransformerEncoder consists of multiple layers of TransformerEncoderLayer. Along with the input sequence, we'll use an attention mask so that the self-attention layers in the TransformerEncoder are only allowed to attend the earlier positions in the sequence (in this language modeling task, tokens in the future positions should be masked). To form output words, the output of the TransformerEncoder model is sent to a final Linear layer, which is followed by a log-Softmax function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5, 10 points\n",
    "\n",
    "In Lab 13, you implemented a DQN model for tic-tac-toe. You method learned to play against a fairly dumb `expert_action` opponent, however.  Also,\n",
    "DQN has proven to be less stable than other methods such as Double DQN, also discussed in Lab 13.\n",
    "\n",
    "Explain below how you would apply double DQN and self-play to improve your tic-tac-toe agent.\n",
    "Provide pseudocode for the algorithm below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your explanation and pseudocode here.*"
   ]
  },
  {
   "source": [
    "Using double DQN, we will copy another DQN as a target network. The target network will compute next Q value instade of use greedy algorithm which is argmax. This will help our network learn with small step because if we use maximum Q-value, at the beginning, it will cause large positive loss when update weight. The target network will get update along with our policy network.\n",
    "\n",
    "Self-play, in short, is did not west time. Like play to it self, it can learn every move by pretant to be own opponent. This will not require expert_agent."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6, 30 points\n",
    "\n",
    "Based on your existing DQN implementation, implement the double DQN and self-play training method\n",
    "you just described. After some training (don't spend too much time on training -- again, we just want to see that the model can\n",
    "learn), show the result you playing a game against your learned agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for training and playing goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}