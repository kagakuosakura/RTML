{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN\n",
    "\n",
    "CycleGAN is a very popular GAN architecture. It is used to learn transformation between images of different styles.\n",
    "\n",
    "The examples of CycleGAN:\n",
    "\n",
    " - a map between artistic and realistic images,\n",
    " - a transformation between images of horse and zebra,\n",
    " - a transformation between winter image and summer image\n",
    " - FaceApp or DeepFake\n",
    "\n",
    "Assume that X is a set of images of horse and Y is a set of images of zebra.\n",
    "\n",
    "The goal of CycleGAN is to learn a mapping function G: X-> Y such that images generated by G(X) are indistinguishable from the image of Y. This objective is achieved using an Adversarial loss. This formulation not only learns G, but it also learns an inverse mapping function F: Y->X and use cycle-consistency loss to enforce **F(G(X)) = X**.\n",
    "\n",
    "While training, 2 kinds of training observations are given as input.\n",
    "\n",
    " - One set of observations have paired images {Xi, Yi} for i where each Xi has it’s Yi counterpart.\n",
    " - The other set of observations has a set of images from X and another set of images from Y without any match between Xi and Yi.\n",
    "\n",
    "As part of Adversarial formulation, there is one Discriminator Dx that classifies whether the transformed Y is indistinguishable from Y. Similarly, there is one more Discriminator Dy that classifies whether  is indistinguishable from X.\n",
    "\n",
    "<img src=\"img/CycleGANmodel.jpg\" title=\"CycleGAN\" style=\"width: 640px;\" />\n",
    "\n",
    "Generator\n",
    "\n",
    "<img src=\"img/CycleGANGenerator.jpg\" title=\"CycleGAN Generator\" style=\"width: 640px;\" />\n",
    "\n",
    "Discriminator\n",
    "\n",
    "<img src=\"img/CycleGANdiscriminator.jpg\" title=\"CycleGAN Discriminator\" style=\"width: 640px;\" />\n",
    "\n",
    "Along with Adversarial Loss, CycleGAN uses cycle-consistency loss to enable training without paired images and this additional loss help the model to minimize reconstruction loss F(G(x)) ≈ X and G(F(Y)) ≈ Y\n",
    "\n",
    "So, All-in-all CycleGAN formulation comprises of 3 individual loss:\n",
    "\n",
    "<img src=\"img/CycleGAN-formulation.png\" title=\"CycleGAN formulation\" style=\"width: 560px;\" />\n",
    "\n",
    "Optimization:\n",
    "\n",
    "<img src=\"img/Optimized-loss-function-CycleGan.png\" title=\"CycleGAN optimization\" style=\"width: 320px;\" />\n",
    "\n",
    "## Results\n",
    "\n",
    "<img src=\"img/CycleGANResultsA2B.jpg\" title=\"CycleGAN Results\" style=\"width: 640px;\" />\n",
    "\n",
    ".\n",
    "\n",
    "<img src=\"img/CycleGANdistortionB2A.jpg\" title=\"CycleGAN Distort\" style=\"width: 640px;\" />\n",
    "\n",
    "In the https://github.com/diegoalejogm/gans/blob/master/CycleGans.ipynb has an examples of CycleGAN from scratch. You can take a look\n",
    "\n",
    "## Get and prepare Cycle GAN implementation\n",
    "\n",
    "Download the Cycle GAN implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, there are two libraries needs to be installed, dominate and visdom. It is used for monitoring the result of training via web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dominate visdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download a data set:\n",
    "\n",
    "(Try a different data set if you like.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd pytorch-CycleGAN-and-pix2pix\n",
    "!./datasets/download_cyclegan_dataset.sh horse2zebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a training run\n",
    "\n",
    "We won't be able to finish a training run in class -- 200 epochs of the horse2zebra dataset with batch size 1 takes about 22 hours on our GPUs. However, we can start a run and see how it goes.\n",
    "\n",
    "We'll also see an example of how to use visdom, which is probably better than matplotlib for visualization when we are running on the server:\n",
    "\n",
    "### In terminal 1:\n",
    "\n",
    "    python -m visdom.server\n",
    "   \n",
    "### In terminal 2 (ssh with parameter -L 8097:localhost:8097):\n",
    "\n",
    "    python train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan\n",
    "\n",
    "## Tips\n",
    "\n",
    "You can understand how to config dataset in CycleGAN.ipynb\n",
    "\n",
    "Configuration commands are in \\options folder\n",
    "\n",
    "You should see your GANs off and running. After every 100 iterations, you should get an update of the different losses and a visualization of results for a real pair (x, y), including G(x), F(y) (fakeB, fakeA), F(G(x), G(F(y)) (recA, recB), F(x), and G(y) (idtB, idtA). If we can all run concurrently you should be getting horsey zebras and somewhat striped horses by the end of lab.\n",
    "\n",
    "## My experiment result from the CycleGAN\n",
    "\n",
    "I had run modern houses and transform to Thai houses. This is one of the result which I tried to train it for 1 day.\n",
    "\n",
    "<img src=\"img/Modern2Thai.png\" title=\"modern2thai\" style=\"width: 640px;\" />\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
